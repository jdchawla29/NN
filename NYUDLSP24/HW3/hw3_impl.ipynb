{"metadata":{"colab":{"provenance":[{"file_id":"1koVK6a2c-W8UhExwMg0GZvC7e8VrfKdr","timestamp":1709414943602},{"file_id":"1XmRlFlmEL5o5F0kZPdnLXTVXlgisZ7AE","timestamp":1679684038081}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Vision Transformer (ViT)\n\nIn this assignment we're going to work with Vision Transformer. We will start to build our own vit model and train it on an image classification task.\nThe purpose of this homework is for you to get familar with ViT and get prepared for the final project.","metadata":{"id":"nQgfvQ4tT-ou"}},{"cell_type":"code","source":"import math\n\nimport torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\nfrom tqdm import tqdm","metadata":{"id":"nFR6WFmfxw43","executionInfo":{"status":"ok","timestamp":1709433080416,"user_tz":300,"elapsed":11385,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:05.111110Z","iopub.execute_input":"2024-03-04T04:15:05.111703Z","iopub.status.idle":"2024-03-04T04:15:08.436634Z","shell.execute_reply.started":"2024-03-04T04:15:05.111669Z","shell.execute_reply":"2024-03-04T04:15:08.435814Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"xGv2wu1MyAPC","executionInfo":{"status":"ok","timestamp":1709433083849,"user_tz":300,"elapsed":172,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"outputId":"8677ec86-eca3-49ed-dd6e-d3c4e659196b","execution":{"iopub.status.busy":"2024-03-04T04:15:08.438414Z","iopub.execute_input":"2024-03-04T04:15:08.438806Z","iopub.status.idle":"2024-03-04T04:15:08.492598Z","shell.execute_reply.started":"2024-03-04T04:15:08.438781Z","shell.execute_reply":"2024-03-04T04:15:08.491500Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# VIT Implementation\n\nThe vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n\nFor the implementation, feel free to experiment different kinds of setup, as long as you use attention as the main computation unit and the ViT can be train to perform the image classification task present later.\nYou can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py","metadata":{"id":"MmNi93C-4rLb"}},{"cell_type":"markdown","source":"## PatchEmbedding\nPatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch.","metadata":{"id":"UNEtT9SQ4jgx"}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size//2 # to compensate for pooling\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n\n        self.num_patches = (image_size // self.patch_size) ** 2\n        self.patch_dim = in_channels * self.patch_size ** 2\n        \n        self.emb = nn.Linear(self.patch_dim, self.embed_dim)\n        \n        self.proj = nn.Sequential(\n            nn.Conv2d(in_channels, embed_dim, kernel_size=self.patch_size, stride=self.patch_size),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(embed_dim, embed_dim, kernel_size=1, stride=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        out = self.proj(x)\n        out = out.flatten(2).transpose(1, 2)\n        \n        return out","metadata":{"id":"rAzsdK5YybDa","executionInfo":{"status":"ok","timestamp":1709435403105,"user_tz":300,"elapsed":220,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:08.494074Z","iopub.execute_input":"2024-03-04T04:15:08.494486Z","iopub.status.idle":"2024-03-04T04:15:08.503905Z","shell.execute_reply.started":"2024-03-04T04:15:08.494428Z","shell.execute_reply":"2024-03-04T04:15:08.502967Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## MultiHeadSelfAttention\n\nThis class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size.","metadata":{"id":"1mk8v66y6MAS"}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n\n        self.q = nn.Linear(embed_dim, embed_dim, bias = False)\n        self.k = nn.Linear(embed_dim, embed_dim, bias = False)\n        self.v = nn.Linear(embed_dim, embed_dim, bias = False)\n\n        self.out = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        b, n, f = x.size()\n        q = self.q(x).view(b, n, self.num_heads, self.embed_dim//self.num_heads).transpose(1,2)\n        k = self.k(x).view(b, n, self.num_heads, self.embed_dim//self.num_heads).transpose(1,2)\n        v = self.v(x).view(b, n, self.num_heads, self.embed_dim//self.num_heads).transpose(1,2)\n\n        attn = F.softmax(torch.einsum(\"bhif, bhjf->bhij\", q, k)/self.embed_dim**0.5, dim=-1)\n        out = torch.einsum(\"bhij, bhjf->bihf\", attn, v)\n        out = self.out(out.flatten(2))\n        return out","metadata":{"id":"V1LeAZq-0dQW","executionInfo":{"status":"ok","timestamp":1709435410777,"user_tz":300,"elapsed":242,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:08.505101Z","iopub.execute_input":"2024-03-04T04:15:08.505383Z","iopub.status.idle":"2024-03-04T04:15:08.515101Z","shell.execute_reply.started":"2024-03-04T04:15:08.505360Z","shell.execute_reply":"2024-03-04T04:15:08.514301Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## TransformerBlock\nThis class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\nYou may also want to use layer normalization or other type of normalization.","metadata":{"id":"NCAURJGJ6jhH"}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n        super(TransformerBlock, self).__init__()\n        self.la1 = nn.LayerNorm(embed_dim,eps=1e-12)\n        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.la2 = nn.LayerNorm(embed_dim,eps=1e-12)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, embed_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        out = self.attention(self.la1(x)) + x\n        out = self.mlp(self.la2(out)) + out\n        return out","metadata":{"id":"0rT15Biv6igC","executionInfo":{"status":"ok","timestamp":1709435413951,"user_tz":300,"elapsed":168,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:08.517942Z","iopub.execute_input":"2024-03-04T04:15:08.518529Z","iopub.status.idle":"2024-03-04T04:15:08.527224Z","shell.execute_reply.started":"2024-03-04T04:15:08.518504Z","shell.execute_reply":"2024-03-04T04:15:08.526389Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## VisionTransformer:\nThis is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes","metadata":{"id":"rgLfJRUm7EDq"}},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n        super(VisionTransformer, self).__init__()\n        num_patches = (image_size // patch_size)**2\n\n        self.patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        self.pos_emb = nn.Parameter(torch.randn(1, num_patches+1, embed_dim))\n        enc_list = [TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)]\n        self.enc = nn.Sequential(*enc_list)\n        self.fc = nn.Sequential(\n            nn.LayerNorm(embed_dim, eps=1e-12),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        out = self.patch_embedding(x)\n        out = torch.cat([self.cls_token.repeat(out.size(0),1,1), out],dim=1)\n        out = out + self.pos_emb\n        out = self.enc(out)\n        out = out[:,0]\n        out = self.fc(out)\n        return out","metadata":{"id":"tgute9Ab0QP4","executionInfo":{"status":"ok","timestamp":1709435416717,"user_tz":300,"elapsed":243,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:08.528431Z","iopub.execute_input":"2024-03-04T04:15:08.528732Z","iopub.status.idle":"2024-03-04T04:15:08.538580Z","shell.execute_reply.started":"2024-03-04T04:15:08.528710Z","shell.execute_reply":"2024-03-04T04:15:08.537679Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Let's train the ViT!\n\nWe will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training","metadata":{"id":"lROdKoO37Uqb"}},{"cell_type":"code","source":"image_size = 128\npatch_size = 16\nin_channels = 3\nembed_dim = 192\nnum_heads = 8\nmlp_dim = 1024\nnum_layers = 8\nnum_classes = 100\ndropout = 0.1\n\nbatch_size = 256","metadata":{"id":"byAC841ix_lb","executionInfo":{"status":"ok","timestamp":1709437860791,"user_tz":300,"elapsed":199,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:08.539389Z","iopub.execute_input":"2024-03-04T04:15:08.539682Z","iopub.status.idle":"2024-03-04T04:15:08.549343Z","shell.execute_reply.started":"2024-03-04T04:15:08.539660Z","shell.execute_reply":"2024-03-04T04:15:08.548580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout)","metadata":{"id":"1V14TFbM8x4l","executionInfo":{"status":"ok","timestamp":1709437868168,"user_tz":300,"elapsed":5224,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"outputId":"dcde73d9-a173-49bd-f53c-52a2530c94eb","execution":{"iopub.status.busy":"2024-03-04T04:15:08.550243Z","iopub.execute_input":"2024-03-04T04:15:08.552110Z","iopub.status.idle":"2024-03-04T04:15:08.611865Z","shell.execute_reply.started":"2024-03-04T04:15:08.552086Z","shell.execute_reply":"2024-03-04T04:15:08.611042Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Using Sharpness-Aware Minimization (SAM) Optimizer for improving generalization (Code: https://github.com/davda54/sam)","metadata":{}},{"cell_type":"code","source":"class SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults.update(self.base_optimizer.defaults)\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                self.state[p][\"old_p\"] = p.data.clone()\n                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self.base_optimizer.param_groups = self.param_groups\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T04:15:08.612895Z","iopub.execute_input":"2024-03-04T04:15:08.613183Z","iopub.status.idle":"2024-03-04T04:15:08.627780Z","shell.execute_reply.started":"2024-03-04T04:15:08.613158Z","shell.execute_reply":"2024-03-04T04:15:08.626857Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nbase_optimizer = torch.optim.SGD\noptimizer = SAM(model.parameters(), base_optimizer, lr=0.1, momentum=0.9, adaptive=True)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.base_optimizer, T_max=50)","metadata":{"id":"4s8-X4l-exSg","executionInfo":{"status":"ok","timestamp":1709437713330,"user_tz":300,"elapsed":220,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"execution":{"iopub.status.busy":"2024-03-04T04:15:08.629044Z","iopub.execute_input":"2024-03-04T04:15:08.629368Z","iopub.status.idle":"2024-03-04T04:15:08.640526Z","shell.execute_reply.started":"2024-03-04T04:15:08.629334Z","shell.execute_reply":"2024-03-04T04:15:08.639712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.Resize(image_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train,);\ntestset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test,);\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"id":"3BOp450mdC-D","executionInfo":{"status":"ok","timestamp":1709437708424,"user_tz":300,"elapsed":7607,"user":{"displayName":"Jaideep Singh Chawla","userId":"18446079774530414160"}},"outputId":"00e05d8d-3f5a-4f9c-91f3-2660ab5ed175","execution":{"iopub.status.busy":"2024-03-04T04:15:08.641537Z","iopub.execute_input":"2024-03-04T04:15:08.641846Z","iopub.status.idle":"2024-03-04T04:15:10.484416Z","shell.execute_reply.started":"2024-03-04T04:15:08.641823Z","shell.execute_reply":"2024-03-04T04:15:10.483373Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# Uncomment if running with Multiple GPUs\n# model = nn.DataParallel(model, device_ids=[0, 1]) \n\nmodel.to(device)\n\nnum_epochs = 30\nbest_val_acc = 0\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for i, data in tqdm(enumerate(trainloader, 0), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", total=len(trainloader), leave=False):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        optimizer.first_step(zero_grad=True)\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.second_step(zero_grad=True)\n        \n        running_loss += loss.item() * inputs.size(0)\n\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(trainloader)\n    epoch_accuracy = 100 * correct / total\n\n    scheduler.step()\n    \n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%, Validation Accuracy: {val_acc:.2f}%\")\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pth\")","metadata":{"id":"eOyk345ve5HN","execution":{"iopub.status.busy":"2024-03-04T04:15:10.485958Z","iopub.execute_input":"2024-03-04T04:15:10.486302Z","iopub.status.idle":"2024-03-04T04:45:16.324167Z","shell.execute_reply.started":"2024-03-04T04:15:10.486270Z","shell.execute_reply":"2024-03-04T04:45:16.323052Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Loss: 1082.6579, Training Accuracy: 5.42%, Validation Accuracy: 8.43%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30, Loss: 967.8433, Training Accuracy: 10.90%, Validation Accuracy: 13.56%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30, Loss: 875.8873, Training Accuracy: 16.66%, Validation Accuracy: 19.29%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30, Loss: 796.5486, Training Accuracy: 22.56%, Validation Accuracy: 24.72%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30, Loss: 734.8067, Training Accuracy: 27.24%, Validation Accuracy: 30.97%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30, Loss: 689.4392, Training Accuracy: 30.87%, Validation Accuracy: 33.43%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30, Loss: 648.8958, Training Accuracy: 34.22%, Validation Accuracy: 36.99%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30, Loss: 616.4746, Training Accuracy: 36.79%, Validation Accuracy: 38.67%\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30, Loss: 586.6314, Training Accuracy: 39.21%, Validation Accuracy: 40.38%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/30, Loss: 560.8058, Training Accuracy: 41.22%, Validation Accuracy: 42.76%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/30, Loss: 538.8508, Training Accuracy: 43.25%, Validation Accuracy: 43.12%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/30, Loss: 514.0950, Training Accuracy: 45.61%, Validation Accuracy: 44.58%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/30, Loss: 491.2745, Training Accuracy: 47.63%, Validation Accuracy: 46.82%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/30, Loss: 472.5086, Training Accuracy: 49.17%, Validation Accuracy: 47.29%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15/30, Loss: 451.6610, Training Accuracy: 51.10%, Validation Accuracy: 47.67%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16/30, Loss: 425.8101, Training Accuracy: 53.59%, Validation Accuracy: 51.19%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17/30, Loss: 404.7301, Training Accuracy: 55.37%, Validation Accuracy: 50.38%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18/30, Loss: 386.2508, Training Accuracy: 57.11%, Validation Accuracy: 52.36%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19/30, Loss: 362.7247, Training Accuracy: 59.42%, Validation Accuracy: 52.67%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20/30, Loss: 344.4425, Training Accuracy: 61.17%, Validation Accuracy: 53.31%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 21/30, Loss: 323.3215, Training Accuracy: 63.15%, Validation Accuracy: 54.28%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 22/30, Loss: 302.9631, Training Accuracy: 65.24%, Validation Accuracy: 54.51%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23/30, Loss: 281.8075, Training Accuracy: 67.51%, Validation Accuracy: 55.01%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 24/30, Loss: 261.9907, Training Accuracy: 69.39%, Validation Accuracy: 54.87%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 25/30, Loss: 245.3479, Training Accuracy: 70.93%, Validation Accuracy: 55.28%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 26/30, Loss: 223.9455, Training Accuracy: 72.91%, Validation Accuracy: 55.92%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 27/30, Loss: 202.3032, Training Accuracy: 75.50%, Validation Accuracy: 56.49%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 28/30, Loss: 182.4770, Training Accuracy: 77.67%, Validation Accuracy: 57.26%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 29/30, Loss: 164.4295, Training Accuracy: 79.62%, Validation Accuracy: 57.11%\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 30/30, Loss: 144.8760, Training Accuracy: 81.91%, Validation Accuracy: 56.92%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Please submit your best_model.pth with this notebook. And report the best test results you get.","metadata":{"id":"-AfNVj1U9xhk"}},{"cell_type":"markdown","source":"As we can observe, the model is already overfitting. With more epochs, it will only get worse at generalization. Vanilla VITs isn't very competitive on CIFAR-100. We need to pass the images through a good feature extractor to have some on inductive bias on locality. The best result here is close to 57%.","metadata":{}}]}